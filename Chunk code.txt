import math
import os
import uuid
from pyspark.sql.functions import monotonically_increasing_id
from pyspark.sql import SparkSession

# Read transactions from Bronze
df = spark.read.format("csv")\
    .option("header", True)\
    .option("inferSchema", True)\
    .load("abfss://DevDolphins_Assignment@onelake.dfs.fabric.microsoft.com/Bronze_Assignment.Lakehouse/Files/raw_data/transactions.csv")
	
display(df)

%%sql
CREATE TABLE IF NOT EXISTS chunk_tracker (
    id INT,
    last_chunk_written INT
)
USING DELTA;

INSERT INTO chunk_tracker VALUES (1, 0);

%%sql
select * from chunk_tracker



from pyspark.sql import Row
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, monotonically_increasing_id

# ----------------------------------------------
# 1. Create `chunk_tracker` table if not exists
# ----------------------------------------------
if not spark.catalog.tableExists("chunk_tracker"):
    print("ðŸ›  Creating 'chunk_tracker' table...")
    initial_df = spark.createDataFrame([Row(id=1, last_chunk_written=0)])
    initial_df.write.mode("overwrite").saveAsTable("chunk_tracker")
else:
    print("âœ… 'chunk_tracker' table already exists.")

# ----------------------------------------------
# 2. Load last written chunk number
# ----------------------------------------------
tracker_df = spark.read.table("chunk_tracker")
last_chunk_written = tracker_df.collect()[0]["last_chunk_written"]
next_chunk_number = last_chunk_written + 1

# ----------------------------------------------
# 3. Load raw transactions CSV from OneLake
# ----------------------------------------------
df = spark.read.format("csv") \
    .option("header", True) \
    .option("inferSchema", True) \
    .load("abfss://DevDolphins_Assignment@onelake.dfs.fabric.microsoft.com/"
          "Bronze_Assignment.Lakehouse/Files/raw_data/transactions.csv")

# ----------------------------------------------
# 4. Assign row numbers for chunking
# ----------------------------------------------
df = df.withColumn("row_id", row_number().over(Window.orderBy(monotonically_increasing_id())))

# ----------------------------------------------
# 5. Select the current chunk
# ----------------------------------------------
chunk_size = 10000
start = (next_chunk_number - 1) * chunk_size + 1
end = start + chunk_size - 1

chunk_df = df.filter((df.row_id >= start) & (df.row_id <= end)).drop("row_id").coalesce(1)

# ----------------------------------------------
# 6. Write chunk to Silver Lakehouse
# ----------------------------------------------
output_path = (f"abfss://DevDolphins_Assignment@onelake.dfs.fabric.microsoft.com/"
               f"Chunks_Silver_Data.Lakehouse/Files/ChunkData/Chunk_Part_{next_chunk_number:02d}")

chunk_df.write.mode("overwrite").option("header", True).csv(output_path)

print(f"âœ… Chunk {next_chunk_number} written to: {output_path}")

# ----------------------------------------------
# 7. Update the tracking table
# ----------------------------------------------
updated_df = spark.createDataFrame([Row(id=1, last_chunk_written=next_chunk_number)])

# Use overwriteSchema to safely overwrite the row
updated_df.write \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .saveAsTable("chunk_tracker")

print(f"ðŸ” chunk_tracker updated with last_chunk_written = {next_chunk_number}")




----------------------------------------------------------------------------------------------------------------------------------------------------


# 1. Create `chunk_tracker` table if not exists
# ----------------------------------------------
if not spark.catalog.tableExists("chunk_tracker"):
    print("ðŸ›  Creating 'chunk_tracker' table...")
    initial_df = spark.createDataFrame([Row(id=1, last_chunk_written=0)])
    initial_df.write.mode("overwrite").saveAsTable("chunk_tracker")
else:
    print("âœ… 'chunk_tracker' table already exists.")
    
    
  
if not spark.catalog.tableExists("chunk_tracker"):   ----(checks whether table exists or not)
initial_df = spark.createDataFrame([Row(id=1, last_chunk_written=0)])   ----(If table is not existed, it creates a dataframe with single row)
initial_df.write.mode("overwrite").saveAsTable("chunk_tracker")   ----(this data frame will save as table)
    




# 2. Load last written chunk number
# ----------------------------------------------
tracker_df = spark.read.table("chunk_tracker")
last_chunk_written = tracker_df.collect()[0]["last_chunk_written"]
next_chunk_number = last_chunk_written + 1




tracker_df = spark.read.table("chunk_tracker")    ----(It reads the table and creates a dataframe)
last_chunk_written = tracker_df.collect()[0]["last_chunk_written"]  ----(It reads the last_chunk_written from the table and stores in that variable)
next_chunk_number = last_chunk_written + 1    ----(It adds +1 the above variable)

Example "last_chunk_written" in "chunk_tracker" is 5
"last_chunk_written" will hold 5
"next_chunk_number" will become 6





# 3. Load raw transactions CSV from OneLake
# ----------------------------------------------
df = spark.read.format("csv") \                    ----(You are telling Spark that the source file is a CSV file)
    .option("header", True) \                      ----(Tells Spark that the first row of the CSV file contains column headers)
    .option("inferSchema", True) \                 ----(Tells Spark to automatically detect data types based on the values in the file)
    .load("abfss://DevDolphins_Assignment@onelake.dfs.fabric.microsoft.com/"
          "Bronze_Assignment.Lakehouse/Files/raw_data/transactions.csv")
                                                    ----(It is a ABFSS URI path points to the sile location)
 
 
 
 
 
# 4. Assign row numbers for chunking
# ----------------------------------------------
df = df.withColumn("row_id", row_number().over(Window.orderBy(monotonically_increasing_id())))
----(It will add row_id column and it is a identity column(monotonically_increasing_id)
      for every row_id row_number is created. so it can assigns sequential numbers)
      
      
      
      
# 5. Select the current chunk
# ----------------------------------------------
chunk_size = 10000
start = (next_chunk_number - 1) * chunk_size + 1
end = start + chunk_size - 1

chunk_df = df.filter((df.row_id >= start) & (df.row_id <= end)).drop("row_id").coalesce(1)


----(Assume
chunk_size = 10000
next_chunk_number = 3
start = (3 - 1) * 10000 + 1 = 20001
end = 20001 + 10000 - 1 = 30000
Select all rows where row_id is from 20,001 to 30,000 and create Chunk3)----

chunk_df = df.filter((df.row_id >= start) & (df.row_id <= end)).drop("row_id").coalesce(1)
----(It filters the above rows. And drop row_id before loading into Chunk files. Spark may create these rows into multiple rows
coalesce(1) will allow to create only one file)




# 6. Write chunk to Silver Lakehouse
# ----------------------------------------------
output_path = (f"abfss://DevDolphins_Assignment@onelake.dfs.fabric.microsoft.com/"
               f"Chunks_Silver_Data.Lakehouse/Files/ChunkData/Chunk_Part_{next_chunk_number:02d}")
                                                       ----(It creates chunk file as Chunk_Part_01, intially next_chunk_number is 0)
chunk_df.write.mode("overwrite").option("header", True).csv(output_path)
                                                       ----(Overwrites the rows into the same file if exists before)
print(f"âœ… Chunk {next_chunk_number} written to: {output_path}")




# 7. Update the tracking table
# ----------------------------------------------
updated_df = spark.createDataFrame([Row(id=1, last_chunk_written=next_chunk_number)])
                                                   ----(Creating df with last_chunk_written=next_chunk_number)
# Use overwriteSchema to safely overwrite the row
updated_df.write \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .saveAsTable("chunk_tracker")
                                                  ----(Overwriting the table with updateddf without effecting schema))
print(f"ðŸ” chunk_tracker updated with last_chunk_written = {next_chunk_number}")